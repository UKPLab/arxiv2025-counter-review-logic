{
    "o_paper": "2023.acl%2023.acl-long.124",
    "cf_paper": {
        "id": "2023.acl%2023.acl-long.124",
        "meta": {
            "venue": "2023.acl",
            "authors": [
                "Shaoxiang Wu",
                "Damai Dai",
                "Ziwei Qin",
                "Tianyu Liu",
                "Binghuai Lin",
                "Yunbo Cao",
                "Zhifang Sui"
            ],
            "categories": [
                "cs.CL"
            ],
            "entry_id": "http://arxiv.org/abs/2305.14652v3",
            "links": [
                "http://arxiv.org/abs/2305.14652v3",
                "http://arxiv.org/pdf/2305.14652v3"
            ],
            "pdf_url": "http://arxiv.org/pdf/2305.14652v3",
            "published": "2023-05-24T02:39:43+00:00",
            "updated": "2023-05-31T08:20:33+00:00",
            "primary_category": "cs.CL",
            "title": "Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion",
            "summary": "Video multimodal fusion aims to integrate multimodal signals in videos, such\nas visual, audio and text, to make a complementary prediction with multiple\nmodalities contents. However, unlike other image-text multimodal tasks, video\nhas longer multimodal sequences with more redundancy and noise in both visual\nand audio modalities. Prior denoising methods like forget gate are coarse in\nthe granularity of noise filtering. They often suppress the redundant and noisy\ninformation at the risk of losing critical information. Therefore, we propose a\ndenoising bottleneck fusion (DBF) model for fine-grained video multimodal\nfusion. On the one hand, we employ a bottleneck mechanism to filter out noise\nand redundancy with a restrained receptive field. On the other hand, we use a\nmutual information maximization module to regulate the filter-out module to\npreserve key information within different modalities. Our DBF model achieves\nsignificant improvement over current state-of-the-art baselines on multiple\nbenchmarks covering multimodal sentiment analysis and multimodal summarization\ntasks. It proves that our model can effectively capture salient features from\nnoisy and redundant video, audio, and text inputs. The code for this paper is\npublicly available at https://github.com/WSXRHFG/DBF.",
            "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
            "venue_config": {
                "id": "2023.acl",
                "name": "Association for Computational Linguistics Conferences of 2023",
                "type": "Association for Computational Linguistics Conference",
                "description": "The ACL (Association for Computational Linguistics) Conference is a premier international conference focused on natural language processing (NLP) and computational linguistics. Computational linguistics is the scientific study of language from a computational perspective. Computational linguists are interested in providing computational models of various kinds of linguistic phenomena. These models may be \"knowledge-based\" (\"hand-crafted\") or \"data-driven\" (\"statistical\" or \"empirical\").",
                "guidelines": "## Review Guidelines:\n\n1. Be Specific:\n\tFlag issues with clear, detailed examples (e.g., \"X misses factor Y\" instead of \"X is wrong\").\n\tEnsure your points are understandable without requiring additional context.\n\n2. Avoid Common Review Pitfalls:\n\t* Avoid overused heuristics like \"Results are not surprising\" or \"The paper lacks novelty\" without clear justification.\n\t* Be open-minded about unexpected results, simpler methods, and niche topics.\n\n3. Watch for Common Issues in NLP Papers:\n\t* Methodology: Ensure valid evaluation, reproducibility, and clear assumptions.\n\t* Artifacts: Check for ethics issues and clear licensing.\n\t* Experimental Results: Avoid misleading framing, unsubstantiated claims, or statistical flaws.\n\t* General: Ensure clear research questions, proper citations, and accurate terminology.\n\n4. Ensure Consistency in Scores:\n\t* Justify low soundness scores with detailed explanations.\n\t* Use excitement scores for personal preferences and overall assessment for the paper\u2019s recommendation.\n\n5. Maintain a Professional, Neutral Tone:\n\t* Write reviews that help authors improve, avoiding sarcasm or dismissiveness.\n\t* Treat authors with respect, considering they may be early-career researchers or under stress.",
                "guidelines_detailed": "## Write a strong review\n\n### 1. Be specific\n\nIf you would like to flag any issues, it should be specific and ideally understandable to the chairs without reading the full paper. Let us consider a few examples.\n\n* Instead of \"The paper is missing relevant references\" use \"The paper is missing references XYZ\"\n* Instead of \"X is not clear\" use \"Y and Z are missing from the description of X\"\n* Instead of \"The formulation of X is wrong\" use \"The formulation of X misses the factor Y\"\n* Instead of \"The contribution is not novel\" use \"Highly similar work X and Y has been published 3+ months prior to submission deadline\"\n* Instead of \"The paper is missing recent baselines\" use \"The proposed method should be compared against recent methods X, Y and Z\"\n* Instead of \"X was done in the way Y\" use \"X was done in the way Y which has the disadvantage Z\"\n* Instead of \"The algorithm's interaction with dataset is problematic\" use \"It's possible that when using the decoding (line 723) on the dataset 3 (line 512), there might not be enough training data to rely on the n-best list.\"\n\nThe advantage of the last version is that it can be understood by an area chair (who has expertise in the subarea) without looking for line numbers.\n\n### 2. Check for common review issues\n\nJudging whether a research paper is \u201cgood\u201d is an objectively hard task, and over the past conferences, we collected a list of common problems, which is presented below. Such comments may point at legitimate problems with the paper, but they are not always \u201cweaknesses\u201d. This can happen even to experienced reviewers, and it\u2019s worth checking your review for these problems before submitting.\n| Heuristic  | Why this is problematic |\n|------------|------------------------|\n| **H1. The results are not surprising** | Just because findings seem obvious doesn\u2019t mean they are known or useless. Empirical validation is still valuable. |\n| **H2. The results contradict what I would expect** | Avoid confirmation bias\u2014unexpected results can challenge assumptions and contribute to progress. |\n| **H3. The results are not novel** | If claiming lack of novelty, provide references. Contributions like analysis or reproduction are valid under CFP. |\n| **H4. This has no precedent in the existing literature** | Novel ideas can be harder to publish, but that doesn\u2019t make them invalid. Avoid excessive conservatism. |\n| **H5. The results do not surpass the latest SOTA** | SOTA is not required for a contribution. Improvements in efficiency, interpretability, or fairness also matter. |\n| **H6. The results are negative** | Negative results prevent hype and overclaiming. Knowing what doesn\u2019t work is as important as knowing what does. |\n| **H7. This method is too simple** | Simpler methods are often preferable\u2014effective, robust, and easier to deploy. Complexity isn\u2019t inherently better. |\n| **H8. The paper doesn't use [my preferred methodology]** | NLP includes diverse contributions beyond specific methodologies. Methodological diversity is essential. |\n| **H9. The topic is too niche** | Specialized research can have a significant impact within its subfield and contribute to broader understanding. |\n| **H10. The approach is tested only on [not English]** | Research on any language is valuable, both practically and theoretically, just as English-only studies are. |\n| **H11. The paper has language errors** | Clarity is more important than perfect language. Scientific content matters more than writing style. |\n| **H12. The paper is missing the [reference X]** | Missing key references is a concern only if the work was published 3+ months before the deadline. Otherwise, suggest rather than criticize. |\n| **H13. The authors could also do [extra experiment X]** | Extra experiments are always possible but not always necessary. If critical, justify why. Otherwise, suggest rather than reject. |\n| **H14. The authors should compare to a 'closed' model X** | Closed-model comparisons are useful only if relevant to the claim. Lack of transparency can make them unreliable. |\n| **H15. The authors should have done [X] instead** | Different approaches exist. Only critique if the chosen method prevents answering the research question or is misleading. |\n| **H16. Limitations != weaknesses** | Acknowledging limitations is required; listing them as weaknesses without justification is unfair. Argue why they invalidate the work if relevant. |\n\nIf you have something like the above listed as a \u201cweakness\u201d of the paper, do try to revisit the paper with an open mind. Both the authors and the ACs will be aware of these guidelines and can refer to them in discussion/meta-review.\n\n### 3. Check for common problems in NLP papers\n\nAs a counter to common problems with reviews, there are also common problems with NLP papers. We do ask you to watch out for these and point them out when you see them. Above all else, published research should at least be technically sound and appropriately scoped. As with the above reviewer issues, it\u2019s a case-by-case evaluation: some things in the list below may be appropriate in a given context, given that they are sufficiently motivated. We provide codes for different types of issues (e.g. M1, G2\u2026) that can be referenced in discussions.\n\n#### Issues with methodology (M)\n\n- **M1. LLM-only evaluation without validation**: If LLMs are used as evaluators, their reliability must be validated in this context.  \n\n- **M2. Reproducibility issues**: Are there enough details to reproduce the experiments, including hyperparameters? Is code or data provided for reviewer evaluation? Reproducibility refers to rerunning experiments with similar results, not reimplementing them.  \n\n- **M3. Undisclosed data quality issues**: If the paper provides datasets or samples, any quality issues should be disclosed. If undisclosed issues exist, it's a serious concern.  \n\n- **M4. Unmotivated selection**: The paper should justify its choice of models and benchmarks in relation to its claims. Even if you would have chosen differently, their selection must be reasonable.  \n\n- **M5. Incomplete assumptions or proofs**: Assumptions should be clearly stated, and formal statements (theorems, lemmas, etc.) must be backed by complete, correct proofs, even if placed in an appendix.  \n\n#### Issues with artifact sourcing or release (T)\n\n- **T1. Ethics issues with data collection**: If human data collection is involved, ethical considerations (e.g., approval, compensation) should be addressed. Major concerns can be flagged for ethics review.\n\n- **T2. Unclear license/release terms**: If a dataset or model is released, the terms should be explicitly stated. Reviewers should check clarity, not make legal judgments.  \n\n#### Issues with experimental results (R)\n\n- **R1. Analysis issues**: Issues include misleading statistics, cherry-picked results, or poorly tuned baselines that affect fairness in comparisons.  \n\n- **R2. Inappropriate scope of claims**: Claims should be based on representative samples. Evaluating a few benchmarks does not justify broad statements about reasoning, understanding, or LLM capabilities.  \n\n- **R3. Hypotheses/speculations as conclusions**: Claims must be supported by evidence or marked as speculation, not presented as established facts.  \n\n- **R4. Misleading framing or overclaiming**: Overgeneralized conclusions, such as stating that LLMs \"understand\" language based only on benchmark results, should be avoided.  \n\n- **R5. Missing statistical significance assessment**: Key results should include error bars, confidence intervals, and statistical significance tests, with details on variability and assumptions.  \n\n#### General issues (G)\n\n- **G1. Unclear research question or contribution**: The paper should clearly state the problem it addresses, its contributions, and its limitations in the abstract and introduction.  \n\n- **G2. Reliance on a bad precedent**: Justifying methods by citing flawed or outdated work is problematic. If precedent is questionable, reviewers should clarify their stance.  \n\n- **G3. Missing/misrepresented related work**: Authors should fairly represent previous research, ensuring novelty claims are justified and related work is accurately cited.  \n\n- **G4. Key terms too vague and undefined**: If an important term is unclear or undefined, the paper should specify or reference its meaning.  \n\n- **G5. Misleading/incorrect citations**: Citations must accurately reflect the referenced work. Increasing reliance on LLM-generated summaries makes this issue more prevalent.  \n\n\n### 4. Check that your scores reflect the review\n\nARR reviewers are asked to provide three scores: soundness, excitement, and overall recommendation.\n\nThe soundness scores must be justified by the text of the review . If you give a low Soundness score without finding any major faults, this means that your review is not a faithful explanation of your recommendation, and you need to revise it. One possible reason for low soundness scores without sufficient justification could be reliance on some unconscious bias or heuristic, like the ones listed in the previous section. Likewise, low reproducibility scores should be justified.\n\nSometimes you may find the work sound, but not something that is interesting for you personally. Such preferences are more subjective and should be reflected in a separate excitement score, which is orthogonal to the assessment of the soundness of the paper. Excitement scores reflect our personal taste, and so do not necessarily come with explicit reasons.\n\nThe Overall assessment score is an explicit recommendation for the outcome of this paper, if it were committed to an *ACL venue. This is a composite score reflecting your assessment of soundness, excitement, and also other factors like novelty and impact. All papers recommended for Findings and main conferences are expected to be sufficiently sound and reproducible, but you may consider a paper worthy of the main conference even if you personally are not excited about it. For example, improvements in efficiency of some algorithm, or creating a high-quality resource for a language/domain that does not yet have resources of that type may not sound very novel or exciting, but you may still consider it a significant contribution due to its potential impact (for its target community).\n\n\n### 5. Check that the tone is professional and neutral\n\nAbove all, a good review should be professional and neutral, if not kind. A key goal of peer review is to help the authors improve their papers. As an anonymous reviewer, you are in a position of power, and the written medium makes it easier to be more dismissive and sarcastic than if you were communicating with the authors face-to-face. Unfortunately such reviews are not uncommon.\n\nConsider that you may well be addressing someone who is only starting on the research path. And/or someone struggling with stress and other mental health issues. And/or someone who has been less lucky with their school and advisors. Academic lives are already stressful enough, and we do not need to inflict extra damage on the mental health of our colleagues with sarcastic or insulting reviews. Write the review you would like to get yourself.\n\nThe fact is, authors and reviewers are actually the same people: the conference crowd lining up for coffee, happily chatting with each other about the grand problems even if they do not always agree. Why can\u2019t we do peer review in the same spirit of fun intellectual interaction with colleagues?\n\nJust like the conference coffee break chats, reviewing is in fact, mutually beneficial: in exchange for feedback, the reviewers get to have the first look at cutting-edge research. Just like the coffee breaks, where you do not necessarily meet only the people working in your subsubfield, peer review may expose you to new techniques. Since there is careful written communication in the authors\u2019 response, you may even find that you were wrong about something, which is a much faster way to learn than making the same mistake in your own research.\n\nNot to mention, someone out there is reviewing your papers too. The more rude or dismissive reviews there are, the more of a norm they become, and the higher the chances you will get one yourself in the future.",
                "template_field_semantics": {
                    "summary": "Paper Summary",
                    "strengths": "Summary of Strengths",
                    "weaknesses": "Summary of Weaknesses",
                    "suggestions": "Comments/Suggestions/Typos"
                },
                "review_template": {
                    "Paper Summary": "Describe what this paper is about. This should help action editors and area chairs to understand the topic of the work and highlight any possible misunderstandings.",
                    "Summary of Strengths": "What are the major reasons to publish this paper at a selective *ACL venue? These could include novel and useful methodology, insightful empirical results or theoretical analysis, clear organization of related literature, or any other reason why interested readers of *ACL papers may find the paper useful.",
                    "Summary of Weaknesses": "What are the concerns that you have about the paper that would cause you to favor prioritizing other high-quality papers that are also under consideration for publication? These could include concerns about correctness of the results or argumentation, limited perceived impact of the methods or findings (note that impact can be significant both in broad or in narrow sub-fields), lack of clarity in exposition, or any other reason why interested readers of *ACL papers may gain less from this paper than they would from other papers under consideration. Where possible, please number your concerns so authors may respond to them individually.",
                    "Comments/Suggestions/Typos": "If you have any comments to the authors about how they may improve their paper, other than addressing the concerns above, please list them here."
                },
                "overall_score_name": "Overall Assessment",
                "review_scores": {
                    "Reviewer Confidence": {
                        "meaning": "Please provide a 'confidence score' for your assessment of this submission to indicate how confident you are in your evaluation.",
                        "scores": {
                            "5": "Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work.",
                            "4": "Quite sure. I tried to check the important points carefully. It\u2019s unlikely, though conceivable, that I missed something that should affect my ratings.",
                            "3": "Pretty sure, but there\u2019s a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper\u2019s details, e.g., the math or experimental design.",
                            "2": "Willing to defend my evaluation, but it is fairly likely that I missed some details, didn\u2019t understand some central points, or can\u2019t be sure about the novelty of the work.",
                            "1": "Not my area, or paper is very hard to understand. My evaluation is just an educated guess."
                        }
                    },
                    "Soundness": {
                        "meaning": "Given that this is a short/long paper, is it sufficiently sound and thorough? Does it clearly state scientific claims and provide adequate support for them? For experimental papers: consider the depth and/or breadth of the research questions investigated, technical soundness of experiments, methodological validity of evaluation. For position papers, surveys: consider whether the current state of the field is adequately represented and main counter-arguments acknowledged. For resource papers: consider the data collection methodology, resulting data & the difference from existing resources are described in sufficient detail.",
                        "scores": {
                            "5": "Excellent: This study is one of the most thorough I have seen, given its type.",
                            "4": "Strong: This study provides sufficient support for all of its claims. Some extra experiments could be nice, but not essential.",
                            "3": "Acceptable: This study provides sufficient support for its main claims. Some minor points may need extra support or details.",
                            "2": "Poor: Some of the main claims are not sufficiently supported. There are major technical/methodological problems.",
                            "1": "Major Issues: This study is not yet sufficiently thorough to warrant publication or is not relevant to ACL."
                        }
                    },
                    "Excitement": {
                        "meaning": "How exciting is this paper for you? Excitement is subjective, and does not necessarily follow what is popular in the field. We may perceive papers as transformational/innovative/surprising, e.g. because they present conceptual breakthroughs or evidence challenging common assumptions/methods/datasets/metrics. We may be excited about the possible impact of the paper on some community (not necessarily large or our own), e.g. lowering barriers, reducing costs, enabling new applications. We may be excited for papers that are relevant, inspiring, or useful for our own research. These factors may combine in different ways for different reviewers.",
                        "scores": {
                            "5": "Highly Exciting: I would recommend this paper to others and/or attend its presentation in a conference.",
                            "4": "Exciting: I would mention this paper to others and/or make an effort to attend its presentation in a conference.",
                            "3": "Interesting: I might mention some points of this paper to others and/or attend its presentation in a conference if there\u2019s time.",
                            "2": "Potentially Interesting: this paper does not resonate with me, but it might with others in the *ACL community.",
                            "1": "Not Exciting: this paper does not resonate with me, and I don\u2019t think it would with others in the *ACL community (e.g. it is in no way related to computational processing of language)."
                        }
                    },
                    "Overall Assessment": {
                        "meaning": "If this paper was committed to an *ACL conference, do you believe it should be accepted? If you recommend conference, Findings and or even award consideration, you can still suggest minor revisions (e.g. typos, non-core missing refs, etc.). Outstanding papers should be either fascinating, controversial, surprising, impressive, or potentially field-changing. Awards will be decided based on the camera-ready version of the paper. Main vs Findings papers: the main criteria for Findings are soundness and reproducibility. Conference recommendations may also consider novelty, impact and other factors.",
                        "scores": {
                            "5": "Consider for Award: I think this paper could be considered for an outstanding paper award at an *ACL conference (up to top 2.5% papers).",
                            "4.5": "Borderline Award",
                            "4": "Conference: I think this paper could be accepted to an *ACL conference.",
                            "3.5": "Borderline Conference",
                            "3": "Findings: I think this paper could be accepted to the Findings of the ACL.",
                            "2.5": "Borderline Findings",
                            "2": "Resubmit next cycle: I think this paper needs substantial revisions that can be completed by the next ARR cycle.",
                            "1.5": "Resubmit after next cycle: I think this paper needs substantial revisions that cannot be completed by the next ARR cycle.",
                            "1": "Do not resubmit: This paper has to be fully redone, or it is not relevant to the *ACL community (e.g. it is in no way related to computational processing of language)."
                        }
                    }
                }
            },
            "oid": "2023.acl%2023.acl-long.124",
            "media": "/home/dycke/Projects/CERGS/data/papers/data/2023.acl/2023.acl%2023.acl-long.124/media"
        },
        "md": "\n\n\n# Denoising Bottleneck with Mutual Information Maximization \n\n\nfor Video Multimodal Fusion\n\n\n\n\n\n###### Abstract\n\n\n\n\n\nVideo multimodal fusion aims to integrate multimodal signals in videos, such as visual, audio and text, to make a complementary prediction with multiple modalities contents. However, unlike other image-text multimodal tasks, video has longer multimodal sequences with more redundancy and noise in both visual and audio modalities. Prior denoising methods like forget gate are coarse in the granularity of noise filtering. They often suppress the redundant and noisy information at the risk of losing critical information. Therefore, we propose a denoising bottleneck fusion (DBF) model for fine-grained video multimodal fusion. On the one hand, we employ a bottleneck mechanism to filter out noise and redundancy with a restrained receptive field. On the other hand, we use a mutual information maximization module to regulate the filter-out module to preserve key information within different modalities. Our DBF model achieves significant improvement over current state-of-the-art baselines on multiple benchmarks covering multimodal sentiment analysis and multimodal summarization tasks. It proves that our model can effectively capture salient features from noisy and redundant video, audio, and text inputs. The code for this paper is publicly available at <https://github.com/WSXRHFG/DBF>.  \n\n\n\n\n\n## 1 Introduction\n\n\n\n\n\n\n\n\n\n\n\nWith the rapid development of social platforms and digital devices, more and more videos are flooding our lives, which leads video multimodal fusion an increasingly popular focus of NLP research. Video multimodal fusion aims to integrate the information from two or more modalities (e.g., visual and audio signals) into text for a more comprehensive reasoning. For example, multimodal sentiment analysis (Poria et\u00a0al., [2020](#bib.bib22)) utilizes contrast between transcript and expression to detect sarcam, multimodal summarization (Sanabria et\u00a0al., [2018](#bib.bib23)) complete summary with information only exists in visual signal.  \n\n\n\n\n\nHowever, as shown in the Figure [1](#S1.F1 \"Figure 1 \u2023 1 Introduction \u2023 Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion\"), there exist plenty of redundancy and noise in video multimodal fusion: 1) high similarity across consecutive frames brings *video redundancy*; 2) useless information, such as the distracting background, introduces *frame noise*; 3) weak alignment between visual stream and text also introduces *misalignment noise*. To alleviate the problem of redundancy and noise in video multimodal fusion, Liu et\u00a0al. ([2020](#bib.bib13)) control the flow of redundant and noisy information between multimodal sequences by a fusion forget gate. The fusion forget gate impairs the impact of noise and redundancy in a coarse grain of the whole modality, so it will also filter out some representative information in the filtered modality.  \n\n\n\n\n\nIn order to remove noise and redundancy while preserving critical information in video multimodal fusion, we propose a denoising fusion bottleneck (DBF) model with mutual information maximization (MI-Max). Firstly, inspired by Nagrani et\u00a0al. ([2021](#bib.bib17)), we introduce a bottleneck module to restrict the redundant and noisy information across different modalities. With the bottleneck module, inputs can only attend to low-capacity bottleneck embeddings to exchange information across different modalities, which urges redundant and noisy information to be discarded. Secondly, in order to prevent key information from being filtered out, we adopt the idea of contrastive learning to supervise the learning of our bottleneck module. Specifically, under the noise-contrastive estimation framework (Gutmann and Hyv\u00e4rinen, [2010](#bib.bib5)), for each sample, we treat all the other samples in the same batch as negative ones. Then, we aim to maximize the mutual information between fusion results and each unimodal inputs by distinguishing their similarity scores from negative samples. Two aforementioned modules complement each other, the MI-Max module supervises the fusion bottleneck not to filter out key information, and in turn, the bottleneck reduces irrelevant information in fusion results to facilitate the maximization of mutual information.  \n\n\n\n\n\nWe conduct extensive experiments on three benchmarks spanning two tasks. MOSI (Zadeh et\u00a0al., [2016](#bib.bib36)) and MOSEI (Zadeh et\u00a0al., [2018b](#bib.bib37)) are two datasets for multimodal sentiment analysis. How2 (Sanabria et\u00a0al., [2018](#bib.bib23)) is a benchmark for multimodal summarization. Experimental results show that our model achieves consistent improvements compared with current state-of-the-art methods. Meanwhile, we perform comprehensive ablation experiments to demonstrate the effectiveness of each module. In addition, we visualize the attention regions and tensity to multiple frames to intuitively show the behavior of our model to reduce noise while retaining key information implicitly.  \n\n\n\n\n\nConcretely, we make the following contributions: (i) We propose a denoising bottleneck fusion model for video multimodal fusion, which reduces redundancy and noise while retaining key information. (ii) We achieve new state-of-the-art performance on three benchmarks spanning two video multimodal fusion tasks. (iii) We provide comprehensive ablation studies and qualitative visualization examples to demonstrate the effectiveness of both bottleneck and MI-Max modules.   \n\n\n\n\n\n## 2 Related Work\n\n\n\n\n\nWe briefly overview related work about multimodal fusion and specific multimodal fusion tasks including multimodal summarization and multimodal sentiment analysis.  \n\n\n\n\n\n### 2.1 Video Multimodal Fusion\n\n\n\n\n\nVideo multimodal fusion aims to join and comprehend information from two or more modalities in videos to make a comprehensive prediction. Early fusion model adopted simple network architectures. Zadeh et\u00a0al. ([2017](#bib.bib34)); Liu et\u00a0al. ([2018a](#bib.bib14)) fuse features by matrix operations; and Zadeh et\u00a0al. ([2018a](#bib.bib35)) designed a LSTM-based model to capture both temporal and inter-modal interactions for better fusion. More recently, models influenced by prevalence of Transformer (Vaswani et\u00a0al., [2017](#bib.bib29)) have emerged constantly: Zhang et\u00a0al. ([2019](#bib.bib38)) injected visual information in the decoder of Transformer by cross attention mechanism to do multimodal translation task; Wu et\u00a0al. ([2021](#bib.bib31)) proposed a text-centric multimodal fusion shared private framework for multimodal fusion, which consists of the cross-modal prediction and sentiment regression parts. And now vision-and-language pre-training has become a promising practice to tackle video multimodal fusion tasks. (Sun et\u00a0al., [2019](#bib.bib25)) firstly extend the Transformer structure to video-language pretraining and used three pre-training tasks: masked language prediction, video text matching, masked video prediction.  \n\n\n\n\n\nIn contrast to existing works, we focus on the fundamental characteristic of video: audio and visual inputs in video are redundant and noisy\u00a0(Nagrani et\u00a0al., [2021](#bib.bib17)) so we aim to remove noise and redundancy while preserving critical information.  \n\n\n\n\n\n### 2.2 Video Multimodal Summarization\n\n\n\n\n\nVideo multimodal summarization aims to generate summaries from visual features and corresponding transcripts in videos. In contrast to unimodal summarization, some information\u00a0(e.g., guitar) only exists in the visual modality. Thus, for videos, utilization of both visual and text features is necessary to generate a more comprehensive summary.  \n\n\n\n\n\nFor datasets, Li et\u00a0al. ([2017](#bib.bib11)) introduced a multimodal summarization dataset consisting of 500 videos of news articles in Chinese and English. Sanabria et\u00a0al. ([2018](#bib.bib23)) proposed the How2 dataset consists of 2,000 hours of short instructional videos, each coming with a summary of two to three sentences.  \n\n\n\n\n\nFor models, Liu et\u00a0al. ([2020](#bib.bib13)) proposed a multistage fusion network with a fusion forget gate module, which controls the flow of redundant information between multimodal long sequences. Meanwhile, Yu et\u00a0al. ([2021a](#bib.bib32)) firstly introduced pre-trained language models into multimodal summarization task and experimented with the optimal injection layer of visual features.  \n\n\n\n\n\nWe also reduce redundancy in video like in (Yu et\u00a0al., [2021a](#bib.bib32)). However, we do not impair the impact of noise and redundancy in a coarse grain with forget gate. Instead, we combine fusion bottleneck and MI-Max modules to filter out noise while preserving key information.  \n\n\n\n\n\n### 2.3 Multimodal Sentiment Analysis\n\n\n\n\n\nMultimodal sentiment analysis (MSA) aims to integrate multimodal resources, such as textual, visual, and acoustic information in videos to predict varied human emotions. In contrast to unimodal sentiment analysis, utterance in the real situation sometimes contains sarcasm, which makes it hard to make accurate prediction by a single modality. In addition, information such as expression in vision and tone in acoustic help assist sentiment prediction. Yu et\u00a0al. ([2021b](#bib.bib33)) introduced a multi-label training scheme that generates extra unimodal labels for each modality and concurrently trained with the main task. Han et\u00a0al. ([2021](#bib.bib6)) build up a hierarchical mutual information maximization guided model to improve the fusion outcome as well as the performance in the downstream multimodal sentiment analysis task. Luo et\u00a0al. ([2021](#bib.bib16)) propose a multi-scale fusion method to align different granularity information from multiple modalities in multimodal sentiment analysis.  \n\n\n\n\n\nOur work is fundamentally different from the above work. We do not focus on complex fusion mechanisms, but take the perspective of information in videos, and stress the importance of validity of information within fusion results.  \n\n\n\n\n\n\n\n\n\n\n\n## 3 Methodology\n\n\n\n\n\nOur denoising fusion bottleneck (DBF) model aims to fuse multimodal inputs from videos to make a comprehensive prediction. The overall architecture of DBF is shown in Figure [2](#S2.F2 \"Figure 2 \u2023 2.3 Multimodal Sentiment Analysis \u2023 2 Related Work \u2023 Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion\"). It first employs a fusion bottleneck module with a restrained receptive field to filter out noise and redundancy when fusing different modalities in videos. Then, DBF maximizes mutual information between fusion results and unimodal inputs to supervise the learning of the fusion bottleneck, aiming to preserve more representative information in fusion results.  \n\n\n\n\n\n### 3.1 Problem Definition\n\n\n\n\n\nIn video multimodal fusion tasks, for each video, the input comprises three sequences of encoded features from textual ($t$), visual ($v$), and acoustic ($a$) modalities. These input features are represented as $X_{m}\\in\\mathbb{R}^{l_{m}\\times d_{m}}$, where $m\\in\\{t,v,a\\}$, and $l_{m}$ and $d_{m}$ denote the sequence length and feature dimension for modality $m$, respectively. The goal of DBF is to extract and integrate task-related information from these input representations to form a unified fusion result $Z\\in\\mathbb{R}^{l\\times d}$. In this paper, we evaluate the quality of the fusion result $Z$ on two tasks: video multimodal sentiment analysis and video multimodal summarization.  \n\n\n\n\n\nFor sentiment analysis, we utilize $Z$ to predict the emotional orientation of a video as a discrete category $\\hat{y}$ from a predefined set of candidates $\\mathcal{C}$  \n\n\n\n\n\n|  | $$\\hat{y}=\\operatorname{argmax}_{y_{j}\\in\\mathcal{C}}\\operatorname{P}_{\\Theta}(y_{j}\\mid Z),$$ |  | (1) |\n\n\n| --- | --- | --- | --- |\n\n\n\n\n\nor as a continuous intensity score $\\hat{y}\\in\\mathbb{R}$  \n\n\n\n\n\n|  | $$\\hat{y}=\\operatorname{P}_{\\Theta}(Z),$$ |  | (2) |\n\n\n| --- | --- | --- | --- |\n\n\n\n\n\nwhere $\\Theta$ denotes the model parameters.  \n\n\n\n\n\nFor summarization, we generate a summary sequence $\\hat{S}=(s_{1},...,s_{l})$ based on $Z$:  \n\n\n\n\n\n|  | $$\\hat{S}=\\text{argmax}_{S}\\operatorname{P}_{\\Theta}(S\\mid Z).$$ |  | (3) |\n\n\n| --- | --- | --- | --- |\n\n\n\n\n\n### 3.2 Fusion Bottleneck\n\n\n\n\n\nAs shown in Figure [2](#S2.F2 \"Figure 2 \u2023 2.3 Multimodal Sentiment Analysis \u2023 2 Related Work \u2023 Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion\"), we first employ a fusion bottleneck with a restrained receptive field to perform multimodal fusion and filter out noise and redundancy in videos. Specifically, fusion bottleneck forces cross-modal information flow passes via randomly initialized bottleneck embeddings $B\\in\\mathbb{R}^{{l_{b}\\times d_{m}}}$ with a small sequence length, where $d_{m}$ denotes dimension of features and $l_{b}\\ll l$. The restrained receptive field of $B$ forces model to collate and condense unimodal information before sharing it with the other modalities.  \n\n\n\n\n\nWith a small length $l_{b}$, embedding $B$ acts like a bottleneck in cross-modal interaction. In the fusion bottleneck module, unimodal features cannot directly attend to each other and they can only attend to the bottleneck embeddings $B$ to exchange information in it. Meanwhile, the bottleneck can attend to all of the modalities, which makes information flow across modalities must pass through the bottleneck with a restrained receptive field. The fusion bottleneck module forces the model to condense and collate information and filter out noise and redundancy.  \n\n\n\n\n\nSpecifically, in the fusion bottleneck module, with bottleneck embeddings $B$ and unimodal features $X_{m}$, the fusion result is calculated as follows:  \n\n\n\n\n\n|  | $$[X_{m}^{l+1}||B_{m}^{l+1}]=\\text{Transformer}([X_{m}^{l}||B^{l}]),$$ |  | (4) |\n\n\n| --- | --- | --- | --- |\n\n\n\n\n\n|  | $$B^{l+1}=\\text{Mean}(B_{m}^{l+1}),$$ |  | (5) |\n\n\n| --- | --- | --- | --- |\n\n\n\n\n\nwhere $l$ denotes the layer number and $||$ denotes the concatenation operation. As shown in Equation [4](#S3.E4 \"In 3.2 Fusion Bottleneck \u2023 3 Methodology \u2023 Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion\") and [5](#S3.E5 \"In 3.2 Fusion Bottleneck \u2023 3 Methodology \u2023 Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion\"), each time a Transformer layer is passed, bottleneck embedding $B$ is updated by unimodal features. In turn, unimodal features integrate condensed information from other modalities through bottleneck embeddings $B$. Finally, we output the text features $X_{t}^{L}$ of the last layer $L$, which are injected with condensed visual and audio information, as the fusion result.  \n\n\n\n\n\n### 3.3 Fusion Mutual Information Maximization\n\n\n\n\n\nThe fusion bottleneck module constrains information flow across modalities in order to filter out noise and redundancy. However, it may result in loss of critical information as well when fusion bottleneck selects what information to be shared. To alleviate this issue, we employ a mutual information maximization (MI-Max) module to preserve representative and salient information from redundant modalities in fusion results.  \n\n\n\n\n\nMutual information is a concept from information theory that estimates the relationship between pairs of variables. Through prompting the mutual information between fusion results $Z$ and multimodal inputs $X_{m}$, we can capture modality-invariant cues among modalities (Han et\u00a0al., [2021](#bib.bib6)) and keep key information preserved by regulating the fusion bottleneck module.  \n\n\n\n\n\nSince direct maximization of mutual information for continuous and high-dimensional variables is intractable (Belghazi et\u00a0al., [2018](#bib.bib1)), we instead minimize the lower bound of mutual information as Han et\u00a0al. ([2021](#bib.bib6)) and Oord et\u00a0al. ([2018](#bib.bib18)). To be specific, we first construct an opposite path from $Z$ to predict $X_{m}$ by an MLP $\\mathcal{F}$. Then, to gauge correlation between the prediction and $X_{m}$, we use a normalized similarity function as follows:  \n\n\n\n\n\n|  | $$\\text{sim}(X_{m},Z)=\\text{exp}\\left(\\frac{X_{m}}{\\left\\|X_{m}\\right\\|^{2}}\\odot\\frac{\\mathcal{F}(Z)}{\\left\\|\\mathcal{F}(Z)\\right\\|^{2}}\\right),$$ |  | (6) |\n\n\n| --- | --- | --- | --- |\n\n\n\n\n\nwhere $\\mathcal{F}$ generates a prediction of $X_{m}$ from $Z$, $\\|\\cdot\\|^{2}$ is the Euclidean norm, and $\\odot$ denotes element-wise product. Then, we incorporate this similarity function into the noise-contrastive estimation framework (Gutmann and Hyv\u00e4rinen, [2010](#bib.bib5)) and produce an InfoNCE loss (Oord et\u00a0al., [2018](#bib.bib18)) which reflects the lower bound of the mutual information:  \n\n\n\n\n\n|  | $$\\mathcal{L}_{\\text{NCE}}^{z,m}=-\\mathbb{E}_{X_{m},Z}\\left[\\log\\frac{e^{\\operatorname{sim}\\left(x_{m}^{{+}},\\mathcal{F}(Z)\\right)}}{\\sum_{k=1}^{K}e^{\\operatorname{sim}\\left(\\tilde{x}_{m}^{k},\\mathcal{F}(Z)\\right)}}\\right]$$ |  | (7) |\n\n\n| --- | --- | --- | --- |\n\n\n\n\n\nwhere $\\tilde{x}_{m}=\\left\\{\\tilde{x}^{1},\\ldots,\\tilde{x}^{K}\\right\\}$ is the negative unimodal inputs that are not matched to the fusion result $Z$ in same batch. Finally, we compute loss for all modalities as follows:  \n\n\n\n\n\n|  | $$\\mathcal{L}_{\\text{NCE}}=\\alpha(\\mathcal{L}_{\\text{NCE}}^{z,v}+\\mathcal{L}_{\\text{NCE}}^{z,a}+\\mathcal{L}_{\\text{NCE}}^{z,t})$$ |  | (8) |\n\n\n| --- | --- | --- | --- |\n\n\n\n\n\nwhere $\\alpha$ is a hyper-parameter that controls the impact of MI-Max.  \n\n\n\n\n\nBy minimizing $\\mathcal{L}_{\\text{NCE}}$, on the one hand, we maximize the lower bound of the mutual information between fusion results and unimodal inputs; on the other hand, we encourage fusion results to reversely predict unimodal inputs as well as possible, which prompts retaining of representative and key information from different modalities in fusion results.  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n## 4 Experiments\n\n\n\n\n\n### 4.1 Tasks, Datasets, and Metrics\n\n\n\n\n\nWe evaluate fusion results of DBF on two video multimodal tasks: video multimodal sentiment analysis and video multimodal summarization.  \n\n\n\n\n\n#### Video Multimodal Sentiment Analysis\n\n\n\n\n\nVideo multimodal sentiment analysis is a regression task that aims to collect and tackle data from multiple resources (text, vision and acoustic) to comprehend varied human emotions. We do this task on MOSI (Zadeh et\u00a0al., [2016](#bib.bib36)) and MOSEI (Zadeh et\u00a0al., [2018b](#bib.bib37)) datasets. The MOSI dataset contains 2198 subjective utterance-video segments, which are manually annotated with a continuous opinion score between [-3, 3], where -3/+3 represents strongly negative/positive sentiments. The MOSEI dataset is an improvement over MOSI, which contains 23453 annotated video segments (utterances), from 5000 videos, 1000 distinct speakers and 250 different topics.  \n\n\n\n\n\nFollowing (Hazarika et\u00a0al., [2020](#bib.bib8)), we use the same metric set to evaluate sentiment intensity predictions: MAE (mean absolute error), which is the average of absolute difference value between predictions and labels; Corr (Pearson correlation) that measures the degree of prediction skew; Acc-7 (seven-class classification accuracy) ranging from -3 to 3; Acc-2 (binary classification accuracy) and F1 score computed for positive/negative and non-negative/negative classification results.  \n\n\n\n\n\n#### Video Multimodal Summarization\n\n\n\n\n\nThe summary task aims to generate abstractive summarization with videos and their corresponding transcripts. We set How2 dataset (Sanabria et\u00a0al., [2018](#bib.bib23)) as benchmark for this task, which is a large-scale dataset consists of 79,114 short instructional videos, and each video is accompanied by a human-generated transcript and a short text summary.  \n\n\n\n\n\nFollowing (Yu et\u00a0al., [2021a](#bib.bib32)), to evaluate summarization, we use metrics as follows: ROUGE (Lin and Hovy, [2003](#bib.bib12)) (ROUGE-1, 2, L) and BLEU (Papineni et\u00a0al., [2002](#bib.bib20)) (BLEU-1, 2, 3, 4), which calculate the recall and precision of n-gram overlaps, respectively; METEOR (Denkowski and Lavie, [2011](#bib.bib3)), which evaluates matching degree of word stems, synonyms and paraphrases; CIDEr (Vedantam et\u00a0al., [2015](#bib.bib30)) is an image captioning metric to compute the cosine similarity between TF-IDF weighted n-grams.  \n\n\n\n\n\n### 4.2 Experimental Settings\n\n\n\n\n\nFor sentiment analysis task, we use BERT-base\u00a0(Devlin et\u00a0al., [2018](#bib.bib4)) to encode text input and extract the [CLS] embedding from the last layer. For acoustic and vision, we use COVAREP (Degottex et\u00a0al., [2014](#bib.bib2)) and Facet 111https://imotions.com/platform/ to extract audio and facial expression features. The visual feature dimensions are 47 for MOSI, 35 for MOSEI, and the audio feature dimensions are 74 for both MOSI and MOSEI.  \n\n\n\n\n\nFor summarization, we use BART (Lewis et\u00a0al., [2019](#bib.bib10)) as the feature extractor and inject visual information in the last layer of the BART encoder. For vision, a 2048-dimensional feature representation is extracted for every 16 non-overlapping frames using a 3D ResNeXt-101 model (Hara et\u00a0al., [2018](#bib.bib7)), which is pre-trained on the Kinetics dataset (Kay et\u00a0al., [2017](#bib.bib9)). Details of the hyper-parameters are given in Appendix\u00a0[A](#A1 \"Appendix A Hyper-parameters \u2023 Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion\"). For frameworks and hardware, we use the deep learning framework PyTorch (Paszke et\u00a0al., [2017](#bib.bib21)) and Huggingface 222https://huggingface.co/ to implement our code. We use a single Nvidia GeForce A40 GPU for sentiment analysis experiments and two for summarization.  \n\n\n\n\n\n### 4.3 Overall Results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe compare performance against DBF by considering various baselines as below: For multimodal sentiment analysis, we compare with MulT (Tsai et\u00a0al., [2019](#bib.bib27)), TFN (Zadeh et\u00a0al., [2017](#bib.bib34)), LMF (Liu et\u00a0al., [2018b](#bib.bib15)), MFM (Tsai et\u00a0al., [2018](#bib.bib28)), ICCN (Sun et\u00a0al., [2020](#bib.bib26)), MISA (Hazarika et\u00a0al., [2020](#bib.bib8)), Self-MM (Yu et\u00a0al., [2021b](#bib.bib33)) and MMIM (Han et\u00a0al., [2021](#bib.bib6)). For multimodal summarization, we compare with HA (Palaskar et\u00a0al., [2019](#bib.bib19)) MFFG (Liu et\u00a0al., [2020](#bib.bib13)) VG-GPLMs (Yu et\u00a0al., [2021a](#bib.bib32)). Details of baselines are in Appendix\u00a0[B](#A2 \"Appendix B Baselines \u2023 Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion\"). The comparative results for sentiment analysis are presented in Table [1](#S3.T1 \"Table 1 \u2023 3.3 Fusion Mutual Information Maximization \u2023 3 Methodology \u2023 Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion\") (MOSI) and Table [2](#S3.T2 \"Table 2 \u2023 3.3 Fusion Mutual Information Maximization \u2023 3 Methodology \u2023 Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion\") (MOSEI). Results for summarization are presented in Table [3](#S4.T3 \"Table 3 \u2023 4.3 Overall Results \u2023 4 Experiments \u2023 Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion\") (How2).  \n\n\n\n\n\nWe find that DBF yields better or comparable results to state-of-the-art methods. To elaborate, DBF significantly outperforms state-of-the-art in all metrics on How2 and in most of metrics on MOSI and MOSEI. For other metrics, DBF achieves very closed performance to state-of-the-art. These outcomes preliminarily demonstrate the efficacy of our method in video multimodal fusion.  \n\n\n\n\n\nFrom the results, we can observe that our model achieves more significant performance improvement on summary task than sentiment analysis. There could be two reasons for this: 1) the size of two datasets is small, yet DBF requires a sufficient amount of data to learn noise and redundancy patterns for this type of video. 2) Visual features are extracted by Facet on sentiment analysis task and more 3D ResNeXt-101 on summary task respectively. Compared to sentiment analysis task, summary task employ a more advanced visual extractor and DBF is heavily influenced by the quality of visual features.  \n\n\n\n\n\n### 4.4 Ablation Study\n\n\n\n\n\n#### Effect of Fusion Bottleneck and MI-Max\n\n\n\n\n\nAs shown in Table [4](#S4.T4 \"Table 4 \u2023 4.3 Overall Results \u2023 4 Experiments \u2023 Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion\"), we first remove respectively MI-Max module and exchange fusion bottleneck module with vanilla fusion methods to observe the effects on performance. We observe that fusion bottleneck and MI-Max both help better fusion results, and the combination of them further improves performance, which reflects the necessity of removing noise while maintaining representative information.  \n\n\n\n\n\n#### Effect of Modalities\n\n\n\n\n\nThen we remove one modality at a time to observe the effect on performance. Firstly, we observe that the multimodal combination provides the best performance, indicating that our model can learn complementary information from different modalities. Next, we observe that the performance drops sharply when the language modality is removed. This may be due to the fact that text has higher information density compared to redundant audio and visual modalities. It verifies two things: 1) It is critical to remove noise and redundancy to increase information density of visual and audio modalities when doing fusion. 2) Text-centric fusion results may help improve performance on multimodal summary and sentiment analysis tasks.  \n\n\n\n\n\n#### Effect of Center Modality\n\n\n\n\n\nAs mentioned above, text-centric fusion results tend to perform better as low information intensity and high redundancy in other modalities. Thus, we evaluate fusion results based on acoustic and vision modality respectively on downstream tasks. We observe an obvious decline in performance when audio or visual modality is used as the central modality.  \n\n\n\n\n\n### 4.5 Case Study\n\n\n\n\n\nIn this section, we first calculate standard deviation and normalized entropy over visual attention scores in the Grad-CAM heatmaps (Selvaraju et\u00a0al., [2017](#bib.bib24)) for DBF and baseline method VG-GPLMs (Yu et\u00a0al., [2021a](#bib.bib32)) respectively. These two metrics show the sharpness of visual attention scores, indicating whether the model focuses more on key frames and ignores redundant content. Then, we compute visualizations on Grad-CAM heatmaps acquired before to show the ability of DBF to filter out redundancy and preserve key information.  \n\n\n\n\n\n#### Statistics of Visualization Results\n\n\n\n\n\nGrad-CAM is a visualization method of images, it obtains visualization heatmaps by calculating weights and gradients during backpropagation, and in this paper we extend Grad-CAM to videos. Further, to quantify this sharpness of visual attention, we calculate standard deviation and normalized entropy on Grad-CAM heatmaps over the test split on How2 dataset. For results, DBF gets 0.830, 0.008, baseline gets 0.404, 0.062 in deviation and normalized entropy respectively. DBF holds a higher deviation and lower entropy, which indicates sharper visual attention maps to discriminate redundancy and key frames.   \n\n\n\n\n\n#### Visualization Example\n\n\n\n\n\nFigure [3](#S4.F3 \"Figure 3 \u2023 4.3 Overall Results \u2023 4 Experiments \u2023 Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion\") provides Grad-CAM visualizations of DBF and baseline method. As we can see, DBF has more sharp attention over continuous frames and ignores redundancy while preserving critical information in visual inputs.  \n\n\n\n\n\n## 5 Conclusion\n\n\n\n\n\nIn this paper, we propose a denoising video multimodal fusion system DBF which contains a fusion bottleneck to filter out redundancy with noise, a mutual information module to preserve key information in fusion results. Our model alleviates redundancy and nosie problem in video multimodal fusion and makes full use of all representative information in redundant modalities (vision and acoustic). In the experiments, we show that our model significantly and consistently outperforms state-of-the-art video multimodal models. In addition, we demonstrate that DBF can appropriately select necessary contents and neglect redundancy in video by comprehensive ablation and visualization studies.  \n\n\n\n\n\nIn the future, we will explore the following directions: (1) We will try to extend the proposed DBF model to more multimodal fusion tasks such as humor detection. (2) We will incorporate vision-text pretraining backbones into our DBF model to further improve its performance.  \n\n\n\n\n\n## Limitations\n\n\n\n\n\nFirst, limited by the category of video multimodal fusion tasks, we do not perform experiments on more tasks to better validate the effectiveness of our method, and we hope to extend our model to more various and complete benchmarks in future work. Secondly, as shown in Section\u00a0[4.3](#S4.SS3 \"4.3 Overall Results \u2023 4 Experiments \u2023 Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion\"), our model achieves relatively slight performance improvement on sentiment analysis task. For reasons, our model may be dependent on the scale of datasets to learn noise and redundancy patterns in video, which needs to be further improved and studied.  \n\n\n\n\n\n## Acknowledgement\n\n\n\n\n\nThis paper is supported by the National Key Research and Development Program of China 2020AAA0106700 and NSFC project U19A2065.  \n\n\n\n\n\n## References\n\n\n\n\n\n* Belghazi et\u00a0al. (2018)  Mohamed\u00a0Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R\u00a0Devon Hjelm. 2018.   Mine: mutual information neural estimation.   *arXiv preprint arXiv:1801.04062*. \n\n\n* Degottex et\u00a0al. (2014)  Gilles Degottex, John Kane, Thomas Drugman, Tuomo Raitio, and Stefan Scherer. 2014.   Covarep\u2014a collaborative voice analysis repository for speech technologies.   In *2014 ieee international conference on acoustics, speech and signal processing (icassp)*, pages 960\u2013964. IEEE. \n\n\n* Denkowski and Lavie (2011)  Michael Denkowski and Alon Lavie. 2011.   Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems.   In *Proceedings of the sixth workshop on statistical machine translation*, pages 85\u201391. \n\n\n* Devlin et\u00a0al. (2018)  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.   Bert: Pre-training of deep bidirectional transformers for language understanding.   *arXiv preprint arXiv:1810.04805*. \n\n\n* Gutmann and Hyv\u00e4rinen (2010)  Michael Gutmann and Aapo Hyv\u00e4rinen. 2010.   Noise-contrastive estimation: A new estimation principle for unnormalized statistical models.   In *Proceedings of the thirteenth international conference on artificial intelligence and statistics*, pages 297\u2013304. JMLR Workshop and Conference Proceedings. \n\n\n* Han et\u00a0al. (2021)  Wei Han, Hui Chen, and Soujanya Poria. 2021.   Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis.   *arXiv preprint arXiv:2109.00412*. \n\n\n* Hara et\u00a0al. (2018)  Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. 2018.   Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?   In *Proceedings of the IEEE conference on Computer Vision and Pattern Recognition*, pages 6546\u20136555. \n\n\n* Hazarika et\u00a0al. (2020)  Devamanyu Hazarika, Roger Zimmermann, and Soujanya Poria. 2020.   Misa: Modality-invariant and-specific representations for multimodal sentiment analysis.   In *Proceedings of the 28th ACM international conference on multimedia*, pages 1122\u20131131. \n\n\n* Kay et\u00a0al. (2017)  Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et\u00a0al. 2017.   The kinetics human action video dataset.   *arXiv preprint arXiv:1705.06950*. \n\n\n* Lewis et\u00a0al. (2019)  Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019.   Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.   *arXiv preprint arXiv:1910.13461*. \n\n\n* Li et\u00a0al. (2017)  Haoran Li, Junnan Zhu, Cong Ma, Jiajun Zhang, and Chengqing Zong. 2017.   Multi-modal summarization for asynchronous collection of text, image, audio and video.   In *Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing*, pages 1092\u20131102. \n\n\n* Lin and Hovy (2003)  Chin-Yew Lin and Eduard Hovy. 2003.   Automatic evaluation of summaries using n-gram co-occurrence statistics.   In *Proceedings of the 2003 human language technology conference of the North American chapter of the association for computational linguistics*, pages 150\u2013157. \n\n\n* Liu et\u00a0al. (2020)  Nayu Liu, Xian Sun, Hongfeng Yu, Wenkai Zhang, and Guangluan Xu. 2020.   Multistage fusion with forget gate for multimodal summarization in open-domain videos.   In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 1834\u20131845. \n\n\n* Liu et\u00a0al. (2018a)  Zhun Liu, Ying Shen, Varun\u00a0Bharadhwaj Lakshminarasimhan, Paul\u00a0Pu Liang, Amir Zadeh, and Louis-Philippe Morency. 2018a.   Efficient low-rank multimodal fusion with modality-specific factors.   *arXiv preprint arXiv:1806.00064*. \n\n\n* Liu et\u00a0al. (2018b)  Zhun Liu, Ying Shen, Varun\u00a0Bharadhwaj Lakshminarasimhan, Paul\u00a0Pu Liang, Amir Zadeh, and Louis-Philippe Morency. 2018b.   Efficient low-rank multimodal fusion with modality-specific factors.   *arXiv preprint arXiv:1806.00064*. \n\n\n* Luo et\u00a0al. (2021)  Huaishao Luo, Lei Ji, Yanyong Huang, Bin Wang, Shenggong Ji, and Tianrui Li. 2021.   Scalevlad: Improving multimodal sentiment analysis via multi-scale fusion of locally descriptors.   *arXiv preprint arXiv:2112.01368*. \n\n\n* Nagrani et\u00a0al. (2021)  Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. 2021.   Attention bottlenecks for multimodal fusion.   *Advances in Neural Information Processing Systems*, 34:14200\u201314213. \n\n\n* Oord et\u00a0al. (2018)  Aaron van\u00a0den Oord, Yazhe Li, and Oriol Vinyals. 2018.   Representation learning with contrastive predictive coding.   *arXiv preprint arXiv:1807.03748*. \n\n\n* Palaskar et\u00a0al. (2019)  Shruti Palaskar, Jindrich Libovick\u1ef3, Spandana Gella, and Florian Metze. 2019.   Multimodal abstractive summarization for how2 videos.   *arXiv preprint arXiv:1906.07901*. \n\n\n* Papineni et\u00a0al. (2002)  Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.   Bleu: a method for automatic evaluation of machine translation.   In *Proceedings of the 40th annual meeting of the Association for Computational Linguistics*, pages 311\u2013318. \n\n\n* Paszke et\u00a0al. (2017)  A.\u00a0Paszke, S.\u00a0Gross, S.\u00a0Chintala, G.\u00a0Chanan, E.\u00a0Yang, Z.\u00a0Devito, Z.\u00a0Lin, A.\u00a0Desmaison, L.\u00a0Antiga, and A.\u00a0Lerer. 2017.   Automatic differentiation in pytorch. \n\n\n* Poria et\u00a0al. (2020)  Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, and Rada Mihalcea. 2020.   Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research.   *IEEE Transactions on Affective Computing*. \n\n\n* Sanabria et\u00a0al. (2018)  Ramon Sanabria, Ozan Caglayan, Shruti Palaskar, Desmond Elliott, Lo\u00efc Barrault, Lucia Specia, and Florian Metze. 2018.   How2: a large-scale dataset for multimodal language understanding.   *arXiv preprint arXiv:1811.00347*. \n\n\n* Selvaraju et\u00a0al. (2017)  Ramprasaath\u00a0R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017.   Grad-cam: Visual explanations from deep networks via gradient-based localization.   In *Proceedings of the IEEE international conference on computer vision*, pages 618\u2013626. \n\n\n* Sun et\u00a0al. (2019)  Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. 2019.   Videobert: A joint model for video and language representation learning.   In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*. \n\n\n* Sun et\u00a0al. (2020)  Zhongkai Sun, Prathusha Sarma, William Sethares, and Yingyu Liang. 2020.   Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis.   In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume\u00a034, pages 8992\u20138999. \n\n\n* Tsai et\u00a0al. (2019)  Yao-Hung\u00a0Hubert Tsai, Shaojie Bai, Paul\u00a0Pu Liang, J\u00a0Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019.   Multimodal transformer for unaligned multimodal language sequences.   In *Proceedings of the conference. Association for Computational Linguistics. Meeting*, volume 2019, page 6558. NIH Public Access. \n\n\n* Tsai et\u00a0al. (2018)  Yao-Hung\u00a0Hubert Tsai, Paul\u00a0Pu Liang, Amir Zadeh, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2018.   Learning factorized multimodal representations.   *arXiv preprint arXiv:1806.06176*. \n\n\n* Vaswani et\u00a0al. (2017)  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\u00a0N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017.   Attention is all you need.   *Advances in neural information processing systems*, 30. \n\n\n* Vedantam et\u00a0al. (2015)  Ramakrishna Vedantam, C\u00a0Lawrence\u00a0Zitnick, and Devi Parikh. 2015.   Cider: Consensus-based image description evaluation.   In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 4566\u20134575. \n\n\n* Wu et\u00a0al. (2021)  Yang Wu, Zijie Lin, Yanyan Zhao, Bing Qin, and Li-Nan Zhu. 2021.   A text-centered shared-private framework via cross-modal prediction for multimodal sentiment analysis.   In *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*, pages 4730\u20134738. \n\n\n* Yu et\u00a0al. (2021a)  Tiezheng Yu, Wenliang Dai, Zihan Liu, and Pascale Fung. 2021a.   Vision guided generative pre-trained language models for multimodal abstractive summarization.   *arXiv preprint arXiv:2109.02401*. \n\n\n* Yu et\u00a0al. (2021b)  Wenmeng Yu, Hua Xu, Ziqi Yuan, and Jiele Wu. 2021b.   Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis.   In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume\u00a035, pages 10790\u201310797. \n\n\n* Zadeh et\u00a0al. (2017)  Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2017.   Tensor fusion network for multimodal sentiment analysis.   *arXiv preprint arXiv:1707.07250*. \n\n\n* Zadeh et\u00a0al. (2018a)  Amir Zadeh, Paul\u00a0Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018a.   Memory fusion network for multi-view sequential learning.   In *Proceedings of the AAAI conference on artificial intelligence*, volume\u00a032. \n\n\n* Zadeh et\u00a0al. (2016)  Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-Philippe Morency. 2016.   Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos.   *arXiv preprint arXiv:1606.06259*. \n\n\n* Zadeh et\u00a0al. (2018b)  AmirAli\u00a0Bagher Zadeh, Paul\u00a0Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018b.   Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph.   In *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 2236\u20132246. \n\n\n* Zhang et\u00a0al. (2019)  Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao. 2019.   Neural machine translation with universal visual representation.   In *International Conference on Learning Representations*. \n\n\n\n\n\n\n\n\n## Figures and Tables\n\n\n\n\n\n[FIGURE S1.F1.g1]\n\n\n![Figure S1.F1.g1](./media/x1.png)\n\n\n\n\n\nFigure 1: \n\n\nAn example of redundancy and noise in a video.\n\n\nAs illustrated, consecutive frames have high cosine similarity, which results in a problem of redundancy.\n\n\nIn addition, useless information like distracting background and weak alignment between frames and transcripts compose noises in videos.\n\n\n[/FIGURE]\n\n\n\n\n\n[FIGURE S2.F2.g1]\n\n\n![Figure S2.F2.g1](./media/x2.png)\n\n\n\n\n\nFigure 2: Overview of our denoising fusion bottleneck (DBF) model. It consists of $N$ Transformer layers to encode videos and texts, and $M$ Transformer layers with fusion bottlenecks for multimodal fusion. We incorporate a mutual information maximization (MI-Max) InfoNCE loss to regulate the bottleneck module, aiming to preserve key information in both modalities from being filtered.\n\n\n[/FIGURE]\n\n\n\n\n\n[FIGURE S4.F3.g1]\n\n\n![Figure S4.F3.g1](./media/x3.png)\n\n\n\n\n\nFigure 3: Comparison of Grad-CAM visualizations of baseline method VG-GPLMs (Yu et\u00a0al., [2021a](#bib.bib32)) (top) and DBF (bottom).\n\n\nIn contrast to even attention to different frames of the baseline method, DBF ignores redundancy and noise in consecutive frames and highly focuses on the key information (*pouring wine* in this example) in a particular frame.\n\n\n[/FIGURE]\n\n\n\n\n\n[TABLE S3.T1]\n\n\n\n\n\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n\n\n<thead class=\"ltx_thead\">\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">MOSI</span></th>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">MAE(<math class=\"ltx_Math\"><semantics><mo>\u2193</mo><annotation-xml><ci>\u2193</ci></annotation-xml><annotation>\\downarrow</annotation></semantics></math>)</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Corr(<math class=\"ltx_Math\"><semantics><mo>\u2191</mo><annotation-xml><ci>\u2191</ci></annotation-xml><annotation>\\uparrow</annotation></semantics></math>)</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Acc-7(<math class=\"ltx_Math\"><semantics><mo>\u2191</mo><annotation-xml><ci>\u2191</ci></annotation-xml><annotation>\\uparrow</annotation></semantics></math>)</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Acc-2(<math class=\"ltx_Math\"><semantics><mo>\u2191</mo><annotation-xml><ci>\u2191</ci></annotation-xml><annotation>\\uparrow</annotation></semantics></math>)</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">F1(<math class=\"ltx_Math\"><semantics><mo>\u2191</mo><annotation-xml><ci>\u2191</ci></annotation-xml><annotation>\\uparrow</annotation></semantics></math>)</span></th>\n\n\n</tr>\n\n\n</thead>\n\n\n<tbody class=\"ltx_tbody\">\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">MulT <cite class=\"ltx_cite ltx_citemacro_citep\">(Tsai et\u00a0al., <a class=\"ltx_ref\">2019</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.871</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.698</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">40.0</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">- / 83.0</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">- / 82.8</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">TFN <cite class=\"ltx_cite ltx_citemacro_citep\">(Zadeh et\u00a0al., <a class=\"ltx_ref\">2017</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.901</td>\n\n\n<td class=\"ltx_td ltx_align_center\">0.698</td>\n\n\n<td class=\"ltx_td ltx_align_center\">34.9</td>\n\n\n<td class=\"ltx_td ltx_align_center\">- / 80.8</td>\n\n\n<td class=\"ltx_td ltx_align_center\">- / 80.7</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">LMF <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et\u00a0al., <a class=\"ltx_ref\">2018b</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.917</td>\n\n\n<td class=\"ltx_td ltx_align_center\">0.695</td>\n\n\n<td class=\"ltx_td ltx_align_center\">33.2</td>\n\n\n<td class=\"ltx_td ltx_align_center\">- / 82.5</td>\n\n\n<td class=\"ltx_td ltx_align_center\">- / 82.4</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MFM <cite class=\"ltx_cite ltx_citemacro_citep\">(Tsai et\u00a0al., <a class=\"ltx_ref\">2018</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.877</td>\n\n\n<td class=\"ltx_td ltx_align_center\">0.706</td>\n\n\n<td class=\"ltx_td ltx_align_center\">35.4</td>\n\n\n<td class=\"ltx_td ltx_align_center\">- / 81.7</td>\n\n\n<td class=\"ltx_td ltx_align_center\">- / 81.6</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">ICCN <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et\u00a0al., <a class=\"ltx_ref\">2020</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.860</td>\n\n\n<td class=\"ltx_td ltx_align_center\">0.710</td>\n\n\n<td class=\"ltx_td ltx_align_center\">39.0</td>\n\n\n<td class=\"ltx_td ltx_align_center\">- / 83.0</td>\n\n\n<td class=\"ltx_td ltx_align_center\">- / 83.0</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MISA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hazarika et\u00a0al., <a class=\"ltx_ref\">2020</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.783</td>\n\n\n<td class=\"ltx_td ltx_align_center\">0.761</td>\n\n\n<td class=\"ltx_td ltx_align_center\">42.3</td>\n\n\n<td class=\"ltx_td ltx_align_center\">81.8 / 83.4</td>\n\n\n<td class=\"ltx_td ltx_align_center\">81.7 / 83.6</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Self-MM <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et\u00a0al., <a class=\"ltx_ref\">2021b</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.712</td>\n\n\n<td class=\"ltx_td ltx_align_center\">0.795</td>\n\n\n<td class=\"ltx_td ltx_align_center\">45.8</td>\n\n\n<td class=\"ltx_td ltx_align_center\">82.5 / 84.8</td>\n\n\n<td class=\"ltx_td ltx_align_center\">82.7 / 84.9</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MMIM<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">\u2020</span></sup> <cite class=\"ltx_cite ltx_citemacro_citep\">(Han et\u00a0al., <a class=\"ltx_ref\">2021</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.700</td>\n\n\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.800</span></td>\n\n\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">46.7</span></td>\n\n\n<td class=\"ltx_td ltx_align_center\">84.2 / 86.1</td>\n\n\n<td class=\"ltx_td ltx_align_center\">84.0 / 86.0</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">DBF</th>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.693</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.801</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">44.8</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">85.1 / 86.9</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">85.1 / 86.9</span></td>\n\n\n</tr>\n\n\n</tbody>\n\n\n</table>\n\n\n\n\n\nTable 1: Results of multimodal sentiment analysis on MOSI. ${\\dagger}$ indicates the previous state-of-the-art model.\n\n\n[/TABLE]\n\n\n\n\n\n[TABLE S3.T2]\n\n\n\n\n\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n\n\n<thead class=\"ltx_thead\">\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">MOSEI</span></th>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">MAE(<math class=\"ltx_Math\"><semantics><mo>\u2193</mo><annotation-xml><ci>\u2193</ci></annotation-xml><annotation>\\downarrow</annotation></semantics></math>)</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Corr(<math class=\"ltx_Math\"><semantics><mo>\u2191</mo><annotation-xml><ci>\u2191</ci></annotation-xml><annotation>\\uparrow</annotation></semantics></math>)</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Acc-7(<math class=\"ltx_Math\"><semantics><mo>\u2191</mo><annotation-xml><ci>\u2191</ci></annotation-xml><annotation>\\uparrow</annotation></semantics></math>)</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">Acc-2(<math class=\"ltx_Math\"><semantics><mo>\u2191</mo><annotation-xml><ci>\u2191</ci></annotation-xml><annotation>\\uparrow</annotation></semantics></math>)</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">F1(<math class=\"ltx_Math\"><semantics><mo>\u2191</mo><annotation-xml><ci>\u2191</ci></annotation-xml><annotation>\\uparrow</annotation></semantics></math>)</span></th>\n\n\n</tr>\n\n\n</thead>\n\n\n<tbody class=\"ltx_tbody\">\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">MulT <cite class=\"ltx_cite ltx_citemacro_citep\">(Tsai et\u00a0al., <a class=\"ltx_ref\">2019</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.580</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.703</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">51.8</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">- / 82.3</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">- / 82.5</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">TFN <cite class=\"ltx_cite ltx_citemacro_citep\">(Zadeh et\u00a0al., <a class=\"ltx_ref\">2017</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.593</td>\n\n\n<td class=\"ltx_td ltx_align_center\">0.700</td>\n\n\n<td class=\"ltx_td ltx_align_center\">50.2</td>\n\n\n<td class=\"ltx_td ltx_align_center\">- / 82.1</td>\n\n\n<td class=\"ltx_td ltx_align_center\">- / 82.5</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">LMF <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et\u00a0al., <a class=\"ltx_ref\">2018b</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.677</td>\n\n\n<td class=\"ltx_td ltx_align_center\">0.695</td>\n\n\n<td class=\"ltx_td ltx_align_center\">48.0</td>\n\n\n<td class=\"ltx_td ltx_align_center\">- / 82.1</td>\n\n\n<td class=\"ltx_td ltx_align_center\">- / 82.0</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MFM <cite class=\"ltx_cite ltx_citemacro_citep\">(Tsai et\u00a0al., <a class=\"ltx_ref\">2018</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.717</td>\n\n\n<td class=\"ltx_td ltx_align_center\">0.706</td>\n\n\n<td class=\"ltx_td ltx_align_center\">51.3</td>\n\n\n<td class=\"ltx_td ltx_align_center\">- / 84.3</td>\n\n\n<td class=\"ltx_td ltx_align_center\">- / 84.4</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">ICCN <cite class=\"ltx_cite ltx_citemacro_citep\">(Sun et\u00a0al., <a class=\"ltx_ref\">2020</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.565</td>\n\n\n<td class=\"ltx_td ltx_align_center\">0.713</td>\n\n\n<td class=\"ltx_td ltx_align_center\">51.6</td>\n\n\n<td class=\"ltx_td ltx_align_center\">- / 84.2</td>\n\n\n<td class=\"ltx_td ltx_align_center\">- / 84.2</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MISA <cite class=\"ltx_cite ltx_citemacro_citep\">(Hazarika et\u00a0al., <a class=\"ltx_ref\">2020</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.555</td>\n\n\n<td class=\"ltx_td ltx_align_center\">0.756</td>\n\n\n<td class=\"ltx_td ltx_align_center\">52.2</td>\n\n\n<td class=\"ltx_td ltx_align_center\">83.8 / 85.3</td>\n\n\n<td class=\"ltx_td ltx_align_center\">83.6 / 85.5</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">Self-MM <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et\u00a0al., <a class=\"ltx_ref\">2021b</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.529</td>\n\n\n<td class=\"ltx_td ltx_align_center\">0.767</td>\n\n\n<td class=\"ltx_td ltx_align_center\">53.5</td>\n\n\n<td class=\"ltx_td ltx_align_center\">82.7 / 85.0</td>\n\n\n<td class=\"ltx_td ltx_align_center\">83.0 / 84.9</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MMIM<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">\u2020</span></sup> <cite class=\"ltx_cite ltx_citemacro_citep\">(Han et\u00a0al., <a class=\"ltx_ref\">2021</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.526</td>\n\n\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">0.772</span></td>\n\n\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">54.2</span></td>\n\n\n<td class=\"ltx_td ltx_align_center\">82.2 / 86.0</td>\n\n\n<td class=\"ltx_td ltx_align_center\">82.7 / 85.9</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">DBF</th>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.523</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.772</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">54.2</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">84.3 / 86.4</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">84.8 / 86.2</span></td>\n\n\n</tr>\n\n\n</tbody>\n\n\n</table>\n\n\n\n\n\nTable 2: Results of multimodal sentiment analysis on MOSEI. ${\\dagger}$ indicates the previous state-of-the-art model.\n\n\n[/TABLE]\n\n\n\n\n\n[TABLE S4.T3]\n\n\n\n\n\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n\n\n<thead class=\"ltx_thead\">\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Method</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">How2</span></th>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">R-1</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">R-2</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">R-L</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">B-1</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">B-2</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">B-3</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">B-4</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">METEOR</span></th>\n\n\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span class=\"ltx_text ltx_font_bold\">CIDEr</span></th>\n\n\n</tr>\n\n\n</thead>\n\n\n<tbody class=\"ltx_tbody\">\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">HA (RNN) <cite class=\"ltx_cite ltx_citemacro_citep\">(Palaskar et\u00a0al., <a class=\"ltx_ref\">2019</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">60.3</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">42.5</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">55.7</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">57.2</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">47.7</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">41.8</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">37.5</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">28.8</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">2.48</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">HA (TF) <cite class=\"ltx_cite ltx_citemacro_citep\">(Palaskar et\u00a0al., <a class=\"ltx_ref\">2019</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">60.2</td>\n\n\n<td class=\"ltx_td ltx_align_center\">43.1</td>\n\n\n<td class=\"ltx_td ltx_align_center\">55.9</td>\n\n\n<td class=\"ltx_td ltx_align_center\">58.6</td>\n\n\n<td class=\"ltx_td ltx_align_center\">48.3</td>\n\n\n<td class=\"ltx_td ltx_align_center\">43.3</td>\n\n\n<td class=\"ltx_td ltx_align_center\">38.1</td>\n\n\n<td class=\"ltx_td ltx_align_center\">28.9</td>\n\n\n<td class=\"ltx_td ltx_align_center\">2.51</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MFFG (RNN) <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et\u00a0al., <a class=\"ltx_ref\">2020</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">62.3</td>\n\n\n<td class=\"ltx_td ltx_align_center\">46.1</td>\n\n\n<td class=\"ltx_td ltx_align_center\">58.2</td>\n\n\n<td class=\"ltx_td ltx_align_center\">59.1</td>\n\n\n<td class=\"ltx_td ltx_align_center\">50.4</td>\n\n\n<td class=\"ltx_td ltx_align_center\">45.1</td>\n\n\n<td class=\"ltx_td ltx_align_center\">41.1</td>\n\n\n<td class=\"ltx_td ltx_align_center\">30.1</td>\n\n\n<td class=\"ltx_td ltx_align_center\">2.69</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MFFG (TF) <cite class=\"ltx_cite ltx_citemacro_citep\">(Liu et\u00a0al., <a class=\"ltx_ref\">2020</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">61.6</td>\n\n\n<td class=\"ltx_td ltx_align_center\">45.1</td>\n\n\n<td class=\"ltx_td ltx_align_center\">57.4</td>\n\n\n<td class=\"ltx_td ltx_align_center\">60.0</td>\n\n\n<td class=\"ltx_td ltx_align_center\">50.9</td>\n\n\n<td class=\"ltx_td ltx_align_center\">45.3</td>\n\n\n<td class=\"ltx_td ltx_align_center\">41.3</td>\n\n\n<td class=\"ltx_td ltx_align_center\">29.9</td>\n\n\n<td class=\"ltx_td ltx_align_center\">2.67</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">VG-GPLMs<sup class=\"ltx_sup\"><span class=\"ltx_text ltx_font_italic\">\u2020</span></sup> <cite class=\"ltx_cite ltx_citemacro_citep\">(Yu et\u00a0al., <a class=\"ltx_ref\">2021a</a>)</cite>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">68.0</td>\n\n\n<td class=\"ltx_td ltx_align_center\">51.4</td>\n\n\n<td class=\"ltx_td ltx_align_center\">63.3</td>\n\n\n<td class=\"ltx_td ltx_align_center\">65.2</td>\n\n\n<td class=\"ltx_td ltx_align_center\">56.3</td>\n\n\n<td class=\"ltx_td ltx_align_center\">50.4</td>\n\n\n<td class=\"ltx_td ltx_align_center\">46.0</td>\n\n\n<td class=\"ltx_td ltx_align_center\">34.0</td>\n\n\n<td class=\"ltx_td ltx_align_center\">3.28</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">DBF</th>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">70.1</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">54.7</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">66.0</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">67.2</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">58.9</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">53.3</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">49.0</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">35.5</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">3.56</span></td>\n\n\n</tr>\n\n\n</tbody>\n\n\n</table>\n\n\n\n\n\nTable 3: Results of multimodal summarization task on How2. The ${\\dagger}$ indicates the previous state-of-the-art model. We denote ROUGE and BLEU by R and B respectively.\n\n\n[/TABLE]\n\n\n\n\n\n[TABLE S4.T4]\n\n\n\n\n\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n\n\n<tbody class=\"ltx_tbody\">\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">Model</span></th>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span class=\"ltx_text ltx_font_bold\">MOSI</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_tt\">\n\n\n<span class=\"ltx_text ltx_font_bold\">MOSEI</span></td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">MAE (<math class=\"ltx_Math\"><semantics><mo>\u2193</mo><annotation-xml><ci>\u2193</ci></annotation-xml><annotation>\\downarrow</annotation></semantics></math>)</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_r\"><span class=\"ltx_text ltx_font_bold\">F1 (<math class=\"ltx_Math\"><semantics><mo>\u2191</mo><annotation-xml><ci>\u2191</ci></annotation-xml><annotation>\\uparrow</annotation></semantics></math>)</span></td>\n\n\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">MAE (<math class=\"ltx_Math\"><semantics><mo>\u2193</mo><annotation-xml><ci>\u2193</ci></annotation-xml><annotation>\\downarrow</annotation></semantics></math>)</span></td>\n\n\n<td class=\"ltx_td ltx_align_center\"><span class=\"ltx_text ltx_font_bold\">F1 (<math class=\"ltx_Math\"><semantics><mo>\u2191</mo><annotation-xml><ci>\u2191</ci></annotation-xml><annotation>\\uparrow</annotation></semantics></math>)</span></td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">1) Ours</th>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.693</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">85.07 / 86.88</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">0.523</span></td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\"><span class=\"ltx_text ltx_font_bold\">84.78 / 86.19</span></td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">2) (-) MI-Max</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.697</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_r\">83.08 / 85.28</td>\n\n\n<td class=\"ltx_td ltx_align_center\">0.536</td>\n\n\n<td class=\"ltx_td ltx_align_center\">80.94 / 85.58</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">3) (-) bottleneck</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.750</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_r\">82.84 / 83.63</td>\n\n\n<td class=\"ltx_td ltx_align_center\">0.537</td>\n\n\n<td class=\"ltx_td ltx_align_center\">77.52 / 83.81</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">4) (-) Language <math class=\"ltx_Math\"><semantics><mi>l</mi><annotation-xml><ci>\ud835\udc59</ci></annotation-xml><annotation>l</annotation></semantics></math>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.391</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">55.54 / 54.95</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.817</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">67.63 / 64.01</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">5) (-) Visual <math class=\"ltx_Math\"><semantics><mi>v</mi><annotation-xml><ci>\ud835\udc63</ci></annotation-xml><annotation>v</annotation></semantics></math>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.700</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_r\">82.78 / 84.33</td>\n\n\n<td class=\"ltx_td ltx_align_center\">0.541</td>\n\n\n<td class=\"ltx_td ltx_align_center\">78.42 / 84.05</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">6) (-) Audio <math class=\"ltx_Math\"><semantics><mi>a</mi><annotation-xml><ci>\ud835\udc4e</ci></annotation-xml><annotation>a</annotation></semantics></math>\n\n\n</th>\n\n\n<td class=\"ltx_td ltx_align_center\">0.720</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_r\">83.02 / 85.86</td>\n\n\n<td class=\"ltx_td ltx_align_center\">0.536</td>\n\n\n<td class=\"ltx_td ltx_align_center\">80.22 / 85.02</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">7) Visual-based</th>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">1.372</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">57.06 / 57.83</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">0.536</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_t\">83.41 / 85.47</td>\n\n\n</tr>\n\n\n<tr class=\"ltx_tr\">\n\n\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\">8) Audio-based</th>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">1.194</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">67.95 / 70.49</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">0.537</td>\n\n\n<td class=\"ltx_td ltx_align_center ltx_border_bb\">83.80 / 85.76</td>\n\n\n</tr>\n\n\n</tbody>\n\n\n</table>\n\n\n\n\n\nTable 4: Results of ablation study. (-) represents removal for the mentioned factors.\n\n\nModel 1 represents the best performing model in each dataset; Model 2,3 presents the effect of MI module and bottleneck module; Model 4,5,6 depicts the effect of individual modalities; Model 7,8 presents the variants of our model as defined in Section [4.4](#S4.SS4 \"4.4 Ablation Study \u2023 4 Experiments \u2023 Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion\").\n\n\n[/TABLE]\n\n\n",
        "structured_md": {
            "title": [
                3,
                3
            ],
            "abstract": [
                12,
                23
            ],
            "sections": {
                "1 Introduction": [
                    24,
                    65
                ],
                "2 Related Work": [
                    66,
                    149
                ],
                "3 Methodology": [
                    150,
                    359
                ],
                "4 Experiments": [
                    360,
                    551
                ],
                "5 Conclusion": [
                    552,
                    569
                ],
                "Limitations": [
                    570,
                    581
                ],
                "Acknowledgement": [
                    582,
                    593
                ],
                "References": [
                    594,
                    719
                ],
                "Figures and Tables": [
                    720,
                    1965
                ]
            },
            "subsections": {
                "1 Introduction": {},
                "2 Related Work": {
                    "2.1 Video Multimodal Fusion": [
                        78,
                        95
                    ],
                    "2.2 Video Multimodal Summarization": [
                        96,
                        125
                    ],
                    "2.3 Multimodal Sentiment Analysis": [
                        126,
                        149
                    ]
                },
                "3 Methodology": {
                    "3.1 Problem Definition": [
                        162,
                        224
                    ],
                    "3.2 Fusion Bottleneck": [
                        225,
                        272
                    ],
                    "3.3 Fusion Mutual Information Maximization": [
                        273,
                        359
                    ]
                },
                "4 Experiments": {
                    "4.1 Tasks, Datasets, and Metrics": [
                        366,
                        377
                    ],
                    "Video Multimodal Sentiment Analysis": [
                        378,
                        395
                    ],
                    "Video Multimodal Summarization": [
                        396,
                        413
                    ],
                    "4.2 Experimental Settings": [
                        414,
                        431
                    ],
                    "4.3 Overall Results": [
                        432,
                        473
                    ],
                    "4.4 Ablation Study": [
                        474,
                        479
                    ],
                    "Effect of Fusion Bottleneck and MI-Max": [
                        480,
                        491
                    ],
                    "Effect of Modalities": [
                        492,
                        503
                    ],
                    "Effect of Center Modality": [
                        504,
                        515
                    ],
                    "4.5 Case Study": [
                        516,
                        527
                    ],
                    "Statistics of Visualization Results": [
                        528,
                        539
                    ],
                    "Visualization Example": [
                        540,
                        551
                    ]
                },
                "5 Conclusion": {},
                "Limitations": {},
                "Acknowledgement": {},
                "References": {},
                "Figures and Tables": {}
            },
            "paragraphs": {
                "1 Introduction": [
                    [
                        36,
                        36
                    ],
                    [
                        42,
                        42
                    ],
                    [
                        48,
                        48
                    ],
                    [
                        54,
                        54
                    ],
                    [
                        60,
                        60
                    ]
                ],
                "2 Related Work": [
                    [
                        72,
                        72
                    ],
                    [
                        84,
                        84
                    ],
                    [
                        90,
                        90
                    ],
                    [
                        102,
                        102
                    ],
                    [
                        108,
                        108
                    ],
                    [
                        114,
                        114
                    ],
                    [
                        120,
                        120
                    ],
                    [
                        132,
                        132
                    ],
                    [
                        138,
                        138
                    ]
                ],
                "3 Methodology": [
                    [
                        156,
                        156
                    ],
                    [
                        168,
                        168
                    ],
                    [
                        174,
                        174
                    ],
                    [
                        180,
                        180
                    ],
                    [
                        183,
                        183
                    ],
                    [
                        189,
                        189
                    ],
                    [
                        195,
                        195
                    ],
                    [
                        198,
                        198
                    ],
                    [
                        204,
                        204
                    ],
                    [
                        210,
                        210
                    ],
                    [
                        216,
                        216
                    ],
                    [
                        219,
                        219
                    ],
                    [
                        231,
                        231
                    ],
                    [
                        237,
                        237
                    ],
                    [
                        243,
                        243
                    ],
                    [
                        249,
                        249
                    ],
                    [
                        252,
                        252
                    ],
                    [
                        258,
                        258
                    ],
                    [
                        261,
                        261
                    ],
                    [
                        267,
                        267
                    ],
                    [
                        279,
                        279
                    ],
                    [
                        285,
                        285
                    ],
                    [
                        291,
                        291
                    ],
                    [
                        297,
                        297
                    ],
                    [
                        300,
                        300
                    ],
                    [
                        306,
                        306
                    ],
                    [
                        312,
                        312
                    ],
                    [
                        315,
                        315
                    ],
                    [
                        321,
                        321
                    ],
                    [
                        327,
                        327
                    ],
                    [
                        330,
                        330
                    ],
                    [
                        336,
                        336
                    ],
                    [
                        342,
                        342
                    ]
                ],
                "4 Experiments": [
                    [
                        372,
                        372
                    ],
                    [
                        384,
                        384
                    ],
                    [
                        390,
                        390
                    ],
                    [
                        402,
                        402
                    ],
                    [
                        408,
                        408
                    ],
                    [
                        420,
                        420
                    ],
                    [
                        426,
                        426
                    ],
                    [
                        456,
                        456
                    ],
                    [
                        462,
                        462
                    ],
                    [
                        468,
                        468
                    ],
                    [
                        486,
                        486
                    ],
                    [
                        498,
                        498
                    ],
                    [
                        510,
                        510
                    ],
                    [
                        522,
                        522
                    ],
                    [
                        534,
                        534
                    ],
                    [
                        546,
                        546
                    ]
                ],
                "5 Conclusion": [
                    [
                        558,
                        558
                    ],
                    [
                        564,
                        564
                    ]
                ],
                "Limitations": [
                    [
                        576,
                        576
                    ]
                ],
                "Acknowledgement": [
                    [
                        588,
                        588
                    ]
                ],
                "References": [
                    [
                        600,
                        600
                    ],
                    [
                        603,
                        603
                    ],
                    [
                        606,
                        606
                    ],
                    [
                        609,
                        609
                    ],
                    [
                        612,
                        612
                    ],
                    [
                        615,
                        615
                    ],
                    [
                        618,
                        618
                    ],
                    [
                        621,
                        621
                    ],
                    [
                        624,
                        624
                    ],
                    [
                        627,
                        627
                    ],
                    [
                        630,
                        630
                    ],
                    [
                        633,
                        633
                    ],
                    [
                        636,
                        636
                    ],
                    [
                        639,
                        639
                    ],
                    [
                        642,
                        642
                    ],
                    [
                        645,
                        645
                    ],
                    [
                        648,
                        648
                    ],
                    [
                        651,
                        651
                    ],
                    [
                        654,
                        654
                    ],
                    [
                        657,
                        657
                    ],
                    [
                        660,
                        660
                    ],
                    [
                        663,
                        663
                    ],
                    [
                        666,
                        666
                    ],
                    [
                        669,
                        669
                    ],
                    [
                        672,
                        672
                    ],
                    [
                        675,
                        675
                    ],
                    [
                        678,
                        678
                    ],
                    [
                        681,
                        681
                    ],
                    [
                        684,
                        684
                    ],
                    [
                        687,
                        687
                    ],
                    [
                        690,
                        690
                    ],
                    [
                        693,
                        693
                    ],
                    [
                        696,
                        696
                    ],
                    [
                        699,
                        699
                    ],
                    [
                        702,
                        702
                    ],
                    [
                        705,
                        705
                    ],
                    [
                        708,
                        708
                    ],
                    [
                        711,
                        711
                    ]
                ],
                "Figures and Tables": []
            },
            "figures": {
                "1 Introduction": {},
                "2 Related Work": {},
                "3 Methodology": {},
                "4 Experiments": {},
                "5 Conclusion": {},
                "Limitations": {},
                "Acknowledgement": {},
                "References": {},
                "Figures and Tables": {
                    "[FIGURE S1.F1.g1]": [
                        726,
                        747
                    ],
                    "[FIGURE S2.F2.g1]": [
                        753,
                        765
                    ],
                    "[FIGURE S4.F3.g1]": [
                        771,
                        786
                    ]
                }
            },
            "tables": {
                "1 Introduction": {},
                "2 Related Work": {},
                "3 Methodology": {},
                "4 Experiments": {},
                "5 Conclusion": {},
                "Limitations": {},
                "Acknowledgement": {},
                "References": {},
                "Figures and Tables": {
                    "[TABLE S3.T1]": [
                        792,
                        1095
                    ],
                    "[TABLE S3.T2]": [
                        1101,
                        1404
                    ],
                    "[TABLE S4.T3]": [
                        1410,
                        1716
                    ],
                    "[TABLE S4.T4]": [
                        1722,
                        1962
                    ]
                }
            },
            "algorithms": {
                "1 Introduction": {},
                "2 Related Work": {},
                "3 Methodology": {},
                "4 Experiments": {},
                "5 Conclusion": {},
                "Limitations": {},
                "Acknowledgement": {},
                "References": {},
                "Figures and Tables": {}
            }
        }
    },
    "changes": [
        "replaced \\n by random number of \\n",
        "moved all figures and tables to the very back of the paper"
    ],
    "counterfactual_type": "paper_layout"
}